{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate wrf LLJ detection with lidar LLJ detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 0.5 m/s data\n",
    "s = pd.read_csv('summary1.csv').drop(columns=['Unnamed: 0'])\n",
    "# s = pd.read_csv('summary1.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with assessment of hits/misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True negative: 8305 (WRF predicts no LLJ, there is no LLJ)\n",
      "True Positive: 43 (WRF predicts an LLJ, there is an LLJ)\n",
      "False positive: 66 (WRF predicts an LLJ, there is no LLJ)\n",
      "False negative: 102 (WRF predicts no LLJ, there is an LLJ)\n",
      "\n",
      "observed LLJs: 145 ; forecasted LLJs: 109\n",
      "observed no: 8371 ; forecasted no: 8407\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(s.lidar_LLJ, s.wrf_LLJ).ravel()\n",
    "tn = 8516-len(s) # no true negatives in dataset\n",
    "print('True negative:', tn, '(WRF predicts no LLJ, there is no LLJ)')\n",
    "print('True Positive:', tp, '(WRF predicts an LLJ, there is an LLJ)')\n",
    "print('False positive:', fp, '(WRF predicts an LLJ, there is no LLJ)')\n",
    "print('False negative:', fn, '(WRF predicts no LLJ, there is an LLJ)')\n",
    "print('')\n",
    "print('observed LLJs:', tp+fn, '; forecasted LLJs:', tp+fp)\n",
    "print('observed no:', tn+fp, '; forecasted no:', tn+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttl = tn+tp+fn+fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy: what fraction of the forecasts were correct?\n",
    "- can be misleading because heavily influenced by most common category (no LLJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.03% of all forecasts were correct\n"
     ]
    }
   ],
   "source": [
    "print(f'{((tp+tn)/ttl)*100:.2f}% of all forecasts were correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias: How did the forecast frequency of \"yes\" events compare to the observed frequency of \"yes\" events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias = 0.7517241379310344\n"
     ]
    }
   ],
   "source": [
    "print(f'bias = {((tp+fp)/(tp+fn))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRF tends to underforecast LLJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of detection (hit rate): What fraction of the observed \"yes\" events were correctly forecast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.66% of all observed LLJs were forecasted\n"
     ]
    }
   ],
   "source": [
    "print(f'{((tp)/(tp+fn))*100:.2f}% of all observed LLJs were forecasted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False alarm ratio: What fraction of the predicted \"yes\" events actually did not occur (i.e., were false alarms)?\n",
    "- should be used with above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.55% of forecasts were false alarms\n"
     ]
    }
   ],
   "source": [
    "print(f'{((fp)/(tp+fp))*100:.2f}% of forecasts were false alarms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of false detection (false alarm rate): What fraction of the observed \"no\" events were incorrectly forecast as \"yes\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79% of observed no LLJs were forecasted as yes\n"
     ]
    }
   ],
   "source": [
    "print(f'{((fp)/(tn+fp))*100:.2f}% of observed no LLJs were forecasted as yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success ratio: What fraction of the forecast \"yes\" events were correctly observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.45% of forecasted LLJs were observed\n"
     ]
    }
   ],
   "source": [
    "print(f'{((tp)/(tp+fp))*100:.2f}% of forecasted LLJs were observed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threat score (critical success index): How well did the forecast \"yes\" events correspond to the observed \"yes\" events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.38% of LLJs were correctly forecasted\n"
     ]
    }
   ],
   "source": [
    "print(f'{((tp)/(tp+fn+fp))*100:.2f}% of LLJs were correctly forecasted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equitable threat score (Gilbert kill score): How well did the forecast \"yes\" events correspond to the observed \"yes\" events (accounting for hits due to chance)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19672601485286104\n"
     ]
    }
   ],
   "source": [
    "hits_rand = ((tp+fn)*(tp+fp)) / ttl\n",
    "print(f'{((tp-hits_rand)/(tp+fn+fp-hits_rand))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some skill in forecasting LLJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hanssen and Kuipers discriminant (true skill statistic, Peirce's skill score): How well did the forecast separate the \"yes\" events from the \"no\" events?\n",
    "- the Hanssen and Kuipers score can also be interpreted as (accuracy for events) + (accuracy for non-events) - 1. For rare events HK is unduly weighted toward the first term (same as POD), so this score may be more useful for more frequent events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2886673614572477\n"
     ]
    }
   ],
   "source": [
    "p1 = tp / (tp+fn)\n",
    "p2 = fp / (fp+tn)\n",
    "print(f'{p1-p2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "res_env",
   "language": "python",
   "name": "res_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
